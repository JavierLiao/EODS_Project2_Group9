{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GR5293 - Proj2 - Group9\n",
    "## NLP with tweets related to COVID\n",
    "#### NLP pipeline with sentiment prediction\n",
    "* Tokenization\n",
    "    > Split text into tokens(sentences or words), for this question, we split the document into sentence for automatic summarization, and words for sentiment analysis and topic modeling\n",
    "* Screen out stop words and other meaningless corpus\n",
    "* Lemmatization\n",
    "    > Here we only use lemmatization rather than stemming is because lemmatization keeps the interpretability of words with their context. While stemming might lead to incorrect meaning. It is important to make morphological analysis of the words. \n",
    "* EDA: wordCloud with different sentiment\n",
    "    > Identify what poeple with different emotions were considering about\n",
    "* EDA: Word2vec with Clustering\n",
    "    > Word2Vec: Effective for detecting the synonymous words or suggesting additional words for a partial sentence\n",
    "\n",
    "    Clustering methods: K-means + DBScan\n",
    "\n",
    "    Use all the words in a specific part-of-speech from all the documents (e.g. all nouns / all adj.s)\n",
    "* (word2vec w/ recommendation)?\n",
    "* Topic Modeling: Feature extraction by TFIDF + Latent Dirichlet Allocation\n",
    "    Build a pipeline with KFoldCV to find the best topic number\n",
    "* Automatic summrization\n",
    "    > Identify what were most people thinking about or tweeting for\n",
    "* Sentiment Analysis: Classification for sentiment(5 classes: Neutral / Positive / Extremely Positive / Negative / Extremely Negative)\n",
    "  \n",
    "    Potential Model: lightGBM / stacking / BERT?\n",
    "#### Preprocessing\n",
    "* One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/kangshuoli/Documents/VScode_workspace/GR5293/EODS-Project2-Group9/EODS_Project2_Group9/doc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%xmode plain\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_filtered</th>\n",
       "      <th>Word_list</th>\n",
       "      <th>Senten_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice talk neighbour family exchange phone nu...</td>\n",
       "      <td>['advice', 'talk', 'neighbour', 'family', 'exc...</td>\n",
       "      <td>['advice talk to your neighbours family to exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>coronavirus australia woolworth give elderly d...</td>\n",
       "      <td>['coronavirus', 'australia', 'woolworth', 'giv...</td>\n",
       "      <td>['coronavirus australia: woolworths to give el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>food stock one empty please dont panic enough ...</td>\n",
       "      <td>['food', 'stock', 'one', 'empty', 'please', 'd...</td>\n",
       "      <td>['my food stock is not the only one which is e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>ready go supermarket covid outbreak im paranoi...</td>\n",
       "      <td>['ready', 'go', 'supermarket', 'covid', 'outbr...</td>\n",
       "      <td>['me, ready to go at supermarket during the #c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>news regionâ  first confirmed covid case came...</td>\n",
       "      <td>['news', 'regionâ', '\\x92', 'first', 'confirme...</td>\n",
       "      <td>['as news of the regionâ\\x92s first confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44248</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>meanwhile supermarket israel people dance sing...</td>\n",
       "      <td>['meanwhile', 'supermarket', 'israel', 'people...</td>\n",
       "      <td>['meanwhile in a supermarket in israel -- peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44249</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>panic buy lot nonperishable item echo need foo...</td>\n",
       "      <td>['panic', 'buy', 'lot', 'nonperishable', 'item...</td>\n",
       "      <td>['did you panic buy a lot of non-perishable it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44250</th>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>asst prof economics talking recent research co...</td>\n",
       "      <td>['asst', 'prof', 'economics', 'talking', 'rece...</td>\n",
       "      <td>[\"asst prof of economics @cconces was on @nbcp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44251</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>gov need somethings instead biar je rakyat ass...</td>\n",
       "      <td>['gov', 'need', 'somethings', 'instead', 'biar...</td>\n",
       "      <td>[\"gov need to do somethings instead of biar je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44252</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>member committed safety employee endusers moni...</td>\n",
       "      <td>['member', 'committed', 'safety', 'employee', ...</td>\n",
       "      <td>['i and @forestandpaper members are committed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44253 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      advice Talk to your neighbours family to excha...            Positive   \n",
       "1      Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "2      My food stock is not the only one which is emp...            Positive   \n",
       "3      Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "4      As news of the regionÂs first confirmed COVID...            Positive   \n",
       "...                                                  ...                 ...   \n",
       "44248  Meanwhile In A Supermarket in Israel -- People...            Positive   \n",
       "44249  Did you panic buy a lot of non-perishable item...            Negative   \n",
       "44250  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral   \n",
       "44251  Gov need to do somethings instead of biar je r...  Extremely Negative   \n",
       "44252  I and @ForestandPaper members are committed to...  Extremely Positive   \n",
       "\n",
       "                                          Tweet_filtered  \\\n",
       "0      advice talk neighbour family exchange phone nu...   \n",
       "1      coronavirus australia woolworth give elderly d...   \n",
       "2      food stock one empty please dont panic enough ...   \n",
       "3      ready go supermarket covid outbreak im paranoi...   \n",
       "4      news regionâ  first confirmed covid case came...   \n",
       "...                                                  ...   \n",
       "44248  meanwhile supermarket israel people dance sing...   \n",
       "44249  panic buy lot nonperishable item echo need foo...   \n",
       "44250  asst prof economics talking recent research co...   \n",
       "44251  gov need somethings instead biar je rakyat ass...   \n",
       "44252  member committed safety employee endusers moni...   \n",
       "\n",
       "                                               Word_list  \\\n",
       "0      ['advice', 'talk', 'neighbour', 'family', 'exc...   \n",
       "1      ['coronavirus', 'australia', 'woolworth', 'giv...   \n",
       "2      ['food', 'stock', 'one', 'empty', 'please', 'd...   \n",
       "3      ['ready', 'go', 'supermarket', 'covid', 'outbr...   \n",
       "4      ['news', 'regionâ', '\\x92', 'first', 'confirme...   \n",
       "...                                                  ...   \n",
       "44248  ['meanwhile', 'supermarket', 'israel', 'people...   \n",
       "44249  ['panic', 'buy', 'lot', 'nonperishable', 'item...   \n",
       "44250  ['asst', 'prof', 'economics', 'talking', 'rece...   \n",
       "44251  ['gov', 'need', 'somethings', 'instead', 'biar...   \n",
       "44252  ['member', 'committed', 'safety', 'employee', ...   \n",
       "\n",
       "                                             Senten_list  \n",
       "0      ['advice talk to your neighbours family to exc...  \n",
       "1      ['coronavirus australia: woolworths to give el...  \n",
       "2      ['my food stock is not the only one which is e...  \n",
       "3      ['me, ready to go at supermarket during the #c...  \n",
       "4      ['as news of the regionâ\\x92s first confirmed ...  \n",
       "...                                                  ...  \n",
       "44248  ['meanwhile in a supermarket in israel -- peop...  \n",
       "44249  ['did you panic buy a lot of non-perishable it...  \n",
       "44250  [\"asst prof of economics @cconces was on @nbcp...  \n",
       "44251  [\"gov need to do somethings instead of biar je...  \n",
       "44252  ['i and @forestandpaper members are committed ...  \n",
       "\n",
       "[44253 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Corona_NLP_filtered.csv\")\n",
    "df.drop(\n",
    "    \"Unnamed: 0\", \n",
    "    axis = 1,\n",
    "    inplace = True\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44253 entries, 0 to 44252\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   OriginalTweet   44253 non-null  object\n",
      " 1   Sentiment       44251 non-null  object\n",
      " 2   Tweet_filtered  44251 non-null  object\n",
      " 3   Word_list       44250 non-null  object\n",
      " 4   Senten_list     44249 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Done in data_cleaning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Analysis\n",
    "1. wordCloud\n",
    "2. word2Vec w/ clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-level / Sentence-level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentence-level automatic summarization\n",
    "* Extractive summarization: pick the original sentence which can represent the main focus of the whole document\n",
    "* Abstractive summarization: generate new sentences for summary\n",
    "    > Purely extractive summaries often times give better results compared to automatic abstractive summaries. This is because of the fact that abstractive summarization methods cope with problems such as semantic representation, inference and natural language generation which are relatively harder than data-driven approaches such as sentence extraction.\n",
    "\n",
    "We use frequency-driven approch here. \n",
    "\n",
    "##### Extractive method w/ TFIDF\n",
    "##### Baseline model: Centroid-based summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  Input \u001b[1;32mIn [10]\u001b[0m in \u001b[1;35m<module>\u001b[0m\n    tfidf_df = tfidf_sum.fit_transform(df[\"Tweet_filtered\"])\n",
      "  File \u001b[1;32m~/miniforge3/envs/eods-s22/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1846\u001b[0m in \u001b[1;35mfit_transform\u001b[0m\n    X = super().fit_transform(raw_documents)\n",
      "  File \u001b[1;32m~/miniforge3/envs/eods-s22/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1202\u001b[0m in \u001b[1;35mfit_transform\u001b[0m\n    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \u001b[1;32m~/miniforge3/envs/eods-s22/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1114\u001b[0m in \u001b[1;35m_count_vocab\u001b[0m\n    for feature in analyze(doc):\n",
      "  File \u001b[1;32m~/miniforge3/envs/eods-s22/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:99\u001b[0m in \u001b[1;35m_analyze\u001b[0m\n    doc = decoder(doc)\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniforge3/envs/eods-s22/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:217\u001b[0;36m in \u001b[0;35mdecode\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise ValueError(\"np.nan is an invalid document, expected byte or \"\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m\u001b[0;31m:\u001b[0m np.nan is an invalid document, expected byte or unicode string.\n"
     ]
    }
   ],
   "source": [
    "# Centroid-based summarization\n",
    "tfidf_sum = TfidfVectorizer(\n",
    "    norm = \"l2\", # The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "    stop_words = None, # already filtered\n",
    "    preprocessor = None, \n",
    "    lowercase = False, # already lowered\n",
    "    max_df = 0.9, \n",
    "    min_df = 10, \n",
    "    ngram_range = (1,5)\n",
    ")\n",
    "tfidf_df = tfidf_sum.fit_transform(df[\"Tweet_filtered\"])\n",
    "vocab_sum = tfidf_sum.vocabulary_\n",
    "\n",
    "# set a threshold to filter the word that are important\n",
    "threshold = 0.6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Document-level sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27646e5cfdb25e10f4fd8ef0a3a49281af2a8af5fb226dbbb4749ab5404e27a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('eods-s22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
