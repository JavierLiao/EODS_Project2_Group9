{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GR5293 - Proj2 - Group9\n",
    "## NLP with tweets related to COVID\n",
    "#### NLP pipeline with sentiment prediction\n",
    "* Tokenization\n",
    "    > Split text into tokens(sentences or words), for this question, we split the document into sentence for automatic summarization, and words for sentiment analysis and topic modeling\n",
    "* Screen out stop words and other meaningless corpus\n",
    "* Lemmatization\n",
    "    > Here we only use lemmatization rather than stemming is because lemmatization keeps the interpretability of words with their context. While stemming might lead to incorrect meaning. It is important to make morphological analysis of the words. \n",
    "* EDA: wordCloud with different sentiment\n",
    "    > Identify what poeple with different emotions were considering about\n",
    "* EDA: Word2vec with Clustering\n",
    "    > Word2Vec: Effective for detecting the synonymous words or suggesting additional words for a partial sentence\n",
    "\n",
    "    Clustering methods: K-means + DBScan\n",
    "\n",
    "    Use all the words in a specific part-of-speech from all the documents (e.g. all nouns / all adj.s)\n",
    "* (word2vec w/ recommendation)?\n",
    "* Topic Modeling: Feature extraction by TFIDF + Latent Dirichlet Allocation\n",
    "    Build a pipeline with KFoldCV to find the best topic number\n",
    "* Automatic summrization\n",
    "    > Identify what were most people thinking about or tweeting for\n",
    "* Sentiment Analysis: Classification for sentiment(5 classes: Neutral / Positive / Extremely Positive / Negative / Extremely Negative)\n",
    "  \n",
    "    Potential Model: lightGBM / stacking / BERT?\n",
    "#### Preprocessing\n",
    "* One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/kangshuoli/Documents/VScode_workspace/GR5293/EODS-Project2-Group9/EODS_Project2_Group9/doc'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%xmode plain\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_filtered</th>\n",
       "      <th>Word_list</th>\n",
       "      <th>Senten_list</th>\n",
       "      <th>Senten_list_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice talk neighbour family exchange phone nu...</td>\n",
       "      <td>['advice', 'talk', 'neighbour', 'family', 'exc...</td>\n",
       "      <td>['advice Talk to your neighbours family to exc...</td>\n",
       "      <td>['advice talk to your neighbours family to exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>coronavirus australia woolworth give elderly d...</td>\n",
       "      <td>['coronavirus', 'australia', 'woolworth', 'giv...</td>\n",
       "      <td>['Coronavirus Australia: Woolworths to give el...</td>\n",
       "      <td>['coronavirus australia woolworths to give eld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>food stock one empty please dont panic enough ...</td>\n",
       "      <td>['food', 'stock', 'one', 'empty', 'please', 'd...</td>\n",
       "      <td>['My food stock is not the only one which is e...</td>\n",
       "      <td>['my food stock is not the only one which is e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>ready go supermarket covid outbreak im paranoi...</td>\n",
       "      <td>['ready', 'go', 'supermarket', 'covid', 'outbr...</td>\n",
       "      <td>['Me, ready to go at supermarket during the #C...</td>\n",
       "      <td>['me ready to go at supermarket during the cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>news regionâ  first confirmed covid case came...</td>\n",
       "      <td>['news', 'regionâ', '\\x92', 'first', 'confirme...</td>\n",
       "      <td>['As news of the regionÂ\\x92s first confirmed ...</td>\n",
       "      <td>['as news of the regionâ\\x92s first confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44248</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>meanwhile supermarket israel people dance sing...</td>\n",
       "      <td>['meanwhile', 'supermarket', 'israel', 'people...</td>\n",
       "      <td>['Meanwhile In A Supermarket in Israel -- Peop...</td>\n",
       "      <td>['meanwhile in a supermarket in israel  people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44249</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>panic buy lot nonperishable item echo need foo...</td>\n",
       "      <td>['panic', 'buy', 'lot', 'nonperishable', 'item...</td>\n",
       "      <td>['Did you panic buy a lot of non-perishable it...</td>\n",
       "      <td>['did you panic buy a lot of nonperishable ite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44250</th>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>asst prof economics talking recent research co...</td>\n",
       "      <td>['asst', 'prof', 'economics', 'talking', 'rece...</td>\n",
       "      <td>[\"Asst Prof of Economics was on talking about ...</td>\n",
       "      <td>['asst prof of economics was on talking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44251</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>gov need somethings instead biar je rakyat ass...</td>\n",
       "      <td>['gov', 'need', 'somethings', 'instead', 'biar...</td>\n",
       "      <td>[\"Gov need to do somethings instead of biar je...</td>\n",
       "      <td>['gov need to do somethings instead of biar je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44252</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>member committed safety employee endusers moni...</td>\n",
       "      <td>['member', 'committed', 'safety', 'employee', ...</td>\n",
       "      <td>['I and members are committed to the safety of...</td>\n",
       "      <td>['i and members are committed to the safety of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44253 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      advice Talk to your neighbours family to excha...            Positive   \n",
       "1      Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "2      My food stock is not the only one which is emp...            Positive   \n",
       "3      Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "4      As news of the regionÂs first confirmed COVID...            Positive   \n",
       "...                                                  ...                 ...   \n",
       "44248  Meanwhile In A Supermarket in Israel -- People...            Positive   \n",
       "44249  Did you panic buy a lot of non-perishable item...            Negative   \n",
       "44250  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral   \n",
       "44251  Gov need to do somethings instead of biar je r...  Extremely Negative   \n",
       "44252  I and @ForestandPaper members are committed to...  Extremely Positive   \n",
       "\n",
       "                                          Tweet_filtered  \\\n",
       "0      advice talk neighbour family exchange phone nu...   \n",
       "1      coronavirus australia woolworth give elderly d...   \n",
       "2      food stock one empty please dont panic enough ...   \n",
       "3      ready go supermarket covid outbreak im paranoi...   \n",
       "4      news regionâ  first confirmed covid case came...   \n",
       "...                                                  ...   \n",
       "44248  meanwhile supermarket israel people dance sing...   \n",
       "44249  panic buy lot nonperishable item echo need foo...   \n",
       "44250  asst prof economics talking recent research co...   \n",
       "44251  gov need somethings instead biar je rakyat ass...   \n",
       "44252  member committed safety employee endusers moni...   \n",
       "\n",
       "                                               Word_list  \\\n",
       "0      ['advice', 'talk', 'neighbour', 'family', 'exc...   \n",
       "1      ['coronavirus', 'australia', 'woolworth', 'giv...   \n",
       "2      ['food', 'stock', 'one', 'empty', 'please', 'd...   \n",
       "3      ['ready', 'go', 'supermarket', 'covid', 'outbr...   \n",
       "4      ['news', 'regionâ', '\\x92', 'first', 'confirme...   \n",
       "...                                                  ...   \n",
       "44248  ['meanwhile', 'supermarket', 'israel', 'people...   \n",
       "44249  ['panic', 'buy', 'lot', 'nonperishable', 'item...   \n",
       "44250  ['asst', 'prof', 'economics', 'talking', 'rece...   \n",
       "44251  ['gov', 'need', 'somethings', 'instead', 'biar...   \n",
       "44252  ['member', 'committed', 'safety', 'employee', ...   \n",
       "\n",
       "                                             Senten_list  \\\n",
       "0      ['advice Talk to your neighbours family to exc...   \n",
       "1      ['Coronavirus Australia: Woolworths to give el...   \n",
       "2      ['My food stock is not the only one which is e...   \n",
       "3      ['Me, ready to go at supermarket during the #C...   \n",
       "4      ['As news of the regionÂ\\x92s first confirmed ...   \n",
       "...                                                  ...   \n",
       "44248  ['Meanwhile In A Supermarket in Israel -- Peop...   \n",
       "44249  ['Did you panic buy a lot of non-perishable it...   \n",
       "44250  [\"Asst Prof of Economics was on talking about ...   \n",
       "44251  [\"Gov need to do somethings instead of biar je...   \n",
       "44252  ['I and members are committed to the safety of...   \n",
       "\n",
       "                                    Senten_list_filtered  \n",
       "0      ['advice talk to your neighbours family to exc...  \n",
       "1      ['coronavirus australia woolworths to give eld...  \n",
       "2      ['my food stock is not the only one which is e...  \n",
       "3      ['me ready to go at supermarket during the cov...  \n",
       "4      ['as news of the regionâ\\x92s first confirmed ...  \n",
       "...                                                  ...  \n",
       "44248  ['meanwhile in a supermarket in israel  people...  \n",
       "44249  ['did you panic buy a lot of nonperishable ite...  \n",
       "44250  ['asst prof of economics was on talking about ...  \n",
       "44251  ['gov need to do somethings instead of biar je...  \n",
       "44252  ['i and members are committed to the safety of...  \n",
       "\n",
       "[44253 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Corona_NLP_filtered.csv\")\n",
    "df.drop(\n",
    "    \"Unnamed: 0\", \n",
    "    axis = 1,\n",
    "    inplace = True\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44253 entries, 0 to 44252\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   OriginalTweet         44253 non-null  object\n",
      " 1   Sentiment             44251 non-null  object\n",
      " 2   Tweet_filtered        44251 non-null  object\n",
      " 3   Word_list             44251 non-null  object\n",
      " 4   Senten_list           44250 non-null  object\n",
      " 5   Senten_list_filtered  44249 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "* Most done in data_cleaning.ipynb\n",
    "* Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(\n",
    "    axis = 0, \n",
    "    how = \"any\", \n",
    "    inplace = True\n",
    ")\n",
    "df.index = np.arange(df.shape[0], dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 44249 entries, 0 to 44248\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   OriginalTweet         44249 non-null  object\n",
      " 1   Sentiment             44249 non-null  object\n",
      " 2   Tweet_filtered        44249 non-null  object\n",
      " 3   Word_list             44249 non-null  object\n",
      " 4   Senten_list           44249 non-null  object\n",
      " 5   Senten_list_filtered  44249 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Analysis\n",
    "1. wordCloud\n",
    "2. word2Vec w/ clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-level / Sentence-level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentence-level automatic summarization\n",
    "* Extractive summarization: pick the original sentence which can represent the main focus of the whole document\n",
    "* Abstractive summarization: generate new sentences for summary\n",
    "    > Purely extractive summaries often times give better results compared to automatic abstractive summaries. This is because of the fact that abstractive summarization methods cope with problems such as semantic representation, inference and natural language generation which are relatively harder than data-driven approaches such as sentence extraction.\n",
    "\n",
    "We use frequency-driven approch here. \n",
    "\n",
    "##### Extractive method w/ TFIDF\n",
    "##### Model: Centroid-based summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid-based summarization\n",
    "\n",
    "# combine all sentences into one list\n",
    "sentence_list = []\n",
    "sentence_list_filtered = []\n",
    "for i in np.arange(df.shape[0]):\n",
    "    sentence_list.extend(df.loc[i, \"Senten_list\"])\n",
    "    sentence_list_filtered.extend(df.loc[i, \"Senten_list_filtered\"])\n",
    "\n",
    "tfidf_sum = TfidfVectorizer(\n",
    "    norm = \"l2\", # The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "    stop_words = None, # already filtered\n",
    "    preprocessor = None, \n",
    "    lowercase = False, # already lowered\n",
    "    max_df = 0.9, \n",
    "    min_df = 10, \n",
    "    ngram_range = (1,5)\n",
    ")\n",
    "tfidf_word = tfidf_sum.fit_transform(df[\"Tweet_filtered\"])\n",
    "vocab_sum = tfidf_sum.get_feature_names()\n",
    "vocab_sum = [x.replace(' ', '_') for x in vocab_sum]\n",
    "\n",
    "\n",
    "# sum all the tfidf score for each word\n",
    "tfidf_word_all_doc = pd.Series(\n",
    "    np.array(tfidf_word.sum(axis = 0)).reshape(-1, 1).ravel(), \n",
    "    index = vocab_sum\n",
    ")\n",
    "\n",
    "# set a threshold to filter out the word that are not important\n",
    "threshold = tfidf_word_all_doc.median()\n",
    "\n",
    "\n",
    "# for each sentence get the sentence centroid score by summing up the word score\n",
    "def get_centroid_score(sent_):\n",
    "    score = 0\n",
    "    word_list = re.split(r'\\s+', sent_)\n",
    "    for word in word_list:\n",
    "        if word in vocab_sum: # get rid of some mis-tokenized words\n",
    "            if tfidf_word_all_doc[word] >= threshold:\n",
    "                score += tfidf_word_all_doc[word]\n",
    "    return score\n",
    "\n",
    "# get the sentence score by filtered sentence\n",
    "sentence_score_dict = {}\n",
    "for i, curr_senten in enumerate(sentence_list_filtered):\n",
    "    sentence_score_dict[sentence_list[i]] = get_centroid_score(curr_senten)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sentence_score_dict\n",
    "import pickle\n",
    "file = open('centroid_based_sentence_score.txt', 'w')\n",
    "pickle.dump(sentence_score_dict, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(sentence_list)):\n",
    "    if sentence_score_dict[sentence_list[i]] >= 200:\n",
    "        print(sentence_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Document-level sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27646e5cfdb25e10f4fd8ef0a3a49281af2a8af5fb226dbbb4749ab5404e27a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('eods-s22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
