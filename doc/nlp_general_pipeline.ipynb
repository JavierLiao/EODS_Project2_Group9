{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GR5293 - Proj2 - Group9\n",
    "## NLP with tweets related to COVID\n",
    "#### NLP pipeline with sentiment prediction\n",
    "* Tokenization\n",
    "    > Split text into tokens(sentences or words), for this question, we split the document into sentence for automatic summarization, and words for sentiment analysis and topic modeling\n",
    "* Screen out stop words and other meaningless corpus\n",
    "* Lemmatization\n",
    "    > Here we only use lemmatization rather than stemming is because lemmatization keeps the interpretability of words with their context. While stemming might lead to incorrect meaning. It is important to make morphological analysis of the words. \n",
    "* EDA: wordCloud with different sentiment\n",
    "    > Identify what poeple with different emotions were considering about\n",
    "* EDA: Word2vec with Clustering\n",
    "    > Word2Vec: Effective for detecting the synonymous words or suggesting additional words for a partial sentence\n",
    "\n",
    "    Clustering methods: K-means + DBScan\n",
    "\n",
    "    Use all the words in a specific part-of-speech from all the documents (e.g. all nouns / all adj.s)\n",
    "* (word2vec w/ recommendation)?\n",
    "* Topic Modeling: Feature extraction by TFIDF + Latent Dirichlet Allocation\n",
    "    Build a pipeline with KFoldCV to find the best topic number\n",
    "* Automatic summrization\n",
    "    > Identify what were most people thinking about or tweeting for\n",
    "* Sentiment Analysis: Classification for sentiment(5 classes: Neutral / Positive / Extremely Positive / Negative / Extremely Negative)\n",
    "  \n",
    "    Potential Model: lightGBM / stacking / BERT?\n",
    "#### Preprocessing\n",
    "* Integer encoding since there's ordinal relation between negative, positive and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/kangshuoli/Documents/VScode_workspace/GR5293/EODS-Project2-Group9/EODS_Project2_Group9/doc'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%xmode plain\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_filtered</th>\n",
       "      <th>Word_list</th>\n",
       "      <th>Senten_list</th>\n",
       "      <th>Senten_list_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice talk neighbour family exchange phone nu...</td>\n",
       "      <td>['advice', 'talk', 'neighbour', 'family', 'exc...</td>\n",
       "      <td>['advice Talk to your neighbours family to exc...</td>\n",
       "      <td>['advice talk to your neighbours family to exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>coronavirus australia woolworth give elderly d...</td>\n",
       "      <td>['coronavirus', 'australia', 'woolworth', 'giv...</td>\n",
       "      <td>['Coronavirus Australia: Woolworths to give el...</td>\n",
       "      <td>['coronavirus australia woolworths to give eld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>food stock one empty please dont panic enough ...</td>\n",
       "      <td>['food', 'stock', 'one', 'empty', 'please', 'd...</td>\n",
       "      <td>['My food stock is not the only one which is e...</td>\n",
       "      <td>['my food stock is not the only one which is e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>ready go supermarket covid outbreak im paranoi...</td>\n",
       "      <td>['ready', 'go', 'supermarket', 'covid', 'outbr...</td>\n",
       "      <td>['Me, ready to go at supermarket during the #C...</td>\n",
       "      <td>['me ready to go at supermarket during the cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>news regionâ  first confirmed covid case came...</td>\n",
       "      <td>['news', 'regionâ', '\\x92', 'first', 'confirme...</td>\n",
       "      <td>['As news of the regionÂ\\x92s first confirmed ...</td>\n",
       "      <td>['as news of the regionâ\\x92s first confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39228</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>meanwhile supermarket israel people dance sing...</td>\n",
       "      <td>['meanwhile', 'supermarket', 'israel', 'people...</td>\n",
       "      <td>['Meanwhile In A Supermarket in Israel -- Peop...</td>\n",
       "      <td>['meanwhile in a supermarket in israel  people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39229</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>panic buy lot nonperishable item echo need foo...</td>\n",
       "      <td>['panic', 'buy', 'lot', 'nonperishable', 'item...</td>\n",
       "      <td>['Did you panic buy a lot of non-perishable it...</td>\n",
       "      <td>['did you panic buy a lot of nonperishable ite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39230</th>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>asst prof economics talking recent research co...</td>\n",
       "      <td>['asst', 'prof', 'economics', 'talking', 'rece...</td>\n",
       "      <td>[\"Asst Prof of Economics was on talking about ...</td>\n",
       "      <td>['asst prof of economics was on talking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39231</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>gov need somethings instead biar je rakyat ass...</td>\n",
       "      <td>['gov', 'need', 'somethings', 'instead', 'biar...</td>\n",
       "      <td>[\"Gov need to do somethings instead of biar je...</td>\n",
       "      <td>['gov need to do somethings instead of biar je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39232</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>member committed safety employee endusers moni...</td>\n",
       "      <td>['member', 'committed', 'safety', 'employee', ...</td>\n",
       "      <td>['I and members are committed to the safety of...</td>\n",
       "      <td>['i and members are committed to the safety of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39233 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      advice Talk to your neighbours family to excha...            Positive   \n",
       "1      Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "2      My food stock is not the only one which is emp...            Positive   \n",
       "3      Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "4      As news of the regionÂs first confirmed COVID...            Positive   \n",
       "...                                                  ...                 ...   \n",
       "39228  Meanwhile In A Supermarket in Israel -- People...            Positive   \n",
       "39229  Did you panic buy a lot of non-perishable item...            Negative   \n",
       "39230  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral   \n",
       "39231  Gov need to do somethings instead of biar je r...  Extremely Negative   \n",
       "39232  I and @ForestandPaper members are committed to...  Extremely Positive   \n",
       "\n",
       "                                          Tweet_filtered  \\\n",
       "0      advice talk neighbour family exchange phone nu...   \n",
       "1      coronavirus australia woolworth give elderly d...   \n",
       "2      food stock one empty please dont panic enough ...   \n",
       "3      ready go supermarket covid outbreak im paranoi...   \n",
       "4      news regionâ  first confirmed covid case came...   \n",
       "...                                                  ...   \n",
       "39228  meanwhile supermarket israel people dance sing...   \n",
       "39229  panic buy lot nonperishable item echo need foo...   \n",
       "39230  asst prof economics talking recent research co...   \n",
       "39231  gov need somethings instead biar je rakyat ass...   \n",
       "39232  member committed safety employee endusers moni...   \n",
       "\n",
       "                                               Word_list  \\\n",
       "0      ['advice', 'talk', 'neighbour', 'family', 'exc...   \n",
       "1      ['coronavirus', 'australia', 'woolworth', 'giv...   \n",
       "2      ['food', 'stock', 'one', 'empty', 'please', 'd...   \n",
       "3      ['ready', 'go', 'supermarket', 'covid', 'outbr...   \n",
       "4      ['news', 'regionâ', '\\x92', 'first', 'confirme...   \n",
       "...                                                  ...   \n",
       "39228  ['meanwhile', 'supermarket', 'israel', 'people...   \n",
       "39229  ['panic', 'buy', 'lot', 'nonperishable', 'item...   \n",
       "39230  ['asst', 'prof', 'economics', 'talking', 'rece...   \n",
       "39231  ['gov', 'need', 'somethings', 'instead', 'biar...   \n",
       "39232  ['member', 'committed', 'safety', 'employee', ...   \n",
       "\n",
       "                                             Senten_list  \\\n",
       "0      ['advice Talk to your neighbours family to exc...   \n",
       "1      ['Coronavirus Australia: Woolworths to give el...   \n",
       "2      ['My food stock is not the only one which is e...   \n",
       "3      ['Me, ready to go at supermarket during the #C...   \n",
       "4      ['As news of the regionÂ\\x92s first confirmed ...   \n",
       "...                                                  ...   \n",
       "39228  ['Meanwhile In A Supermarket in Israel -- Peop...   \n",
       "39229  ['Did you panic buy a lot of non-perishable it...   \n",
       "39230  [\"Asst Prof of Economics was on talking about ...   \n",
       "39231  [\"Gov need to do somethings instead of biar je...   \n",
       "39232  ['I and members are committed to the safety of...   \n",
       "\n",
       "                                    Senten_list_filtered  \n",
       "0      ['advice talk to your neighbours family to exc...  \n",
       "1      ['coronavirus australia woolworths to give eld...  \n",
       "2      ['my food stock is not the only one which is e...  \n",
       "3      ['me ready to go at supermarket during the cov...  \n",
       "4      ['as news of the regionâ\\x92s first confirmed ...  \n",
       "...                                                  ...  \n",
       "39228  ['meanwhile in a supermarket in israel  people...  \n",
       "39229  ['did you panic buy a lot of nonperishable ite...  \n",
       "39230  ['asst prof of economics was on talking about ...  \n",
       "39231  ['gov need to do somethings instead of biar je...  \n",
       "39232  ['i and members are committed to the safety of...  \n",
       "\n",
       "[39233 rows x 6 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Corona_NLP_filtered.csv\")\n",
    "df.drop(\n",
    "    \"Unnamed: 0\", \n",
    "    axis = 1,\n",
    "    inplace = True\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39233 entries, 0 to 39232\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   OriginalTweet         39233 non-null  object\n",
      " 1   Sentiment             39231 non-null  object\n",
      " 2   Tweet_filtered        39231 non-null  object\n",
      " 3   Word_list             39231 non-null  object\n",
      " 4   Senten_list           39230 non-null  object\n",
      " 5   Senten_list_filtered  39229 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "* Most done in data_cleaning.ipynb\n",
    "* Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(\n",
    "    axis = 0, \n",
    "    how = \"any\", \n",
    "    inplace = True\n",
    ")\n",
    "df.index = np.arange(df.shape[0], dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39229 entries, 0 to 39228\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   OriginalTweet         39229 non-null  object\n",
      " 1   Sentiment             39229 non-null  object\n",
      " 2   Tweet_filtered        39229 non-null  object\n",
      " 3   Word_list             39229 non-null  object\n",
      " 4   Senten_list           39229 non-null  object\n",
      " 5   Senten_list_filtered  39229 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Analysis\n",
    "1. wordCloud\n",
    "2. word2Vec w/ clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-level / Sentence-level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentence-level automatic summarization\n",
    "* Extractive summarization: pick the original sentence which can represent the main focus of the whole document\n",
    "* Abstractive summarization: generate new sentences for summary\n",
    "    > Purely extractive summaries often times give better results compared to automatic abstractive summaries. This is because of the fact that abstractive summarization methods cope with problems such as semantic representation, inference and natural language generation which are relatively harder than data-driven approaches such as sentence extraction.\n",
    "\n",
    "We use frequency-driven approch here. \n",
    "\n",
    "##### Extractive method w/ TFIDF\n",
    "##### Model: Centroid-based summarization (simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF threshold: 20.5310\n"
     ]
    }
   ],
   "source": [
    "# Centroid-based summarization\n",
    "\n",
    "# Sentence cleaning\n",
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "def lower(text):\n",
    "    low_text = text.lower()\n",
    "    return low_text\n",
    "\n",
    "def remove_num(text):\n",
    "    remove = re.sub(r'\\d+', '' ,text)\n",
    "    return remove\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    clean_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(clean_list)\n",
    "    return clean_str\n",
    "\n",
    "def remove_at(text):\n",
    "    at = re.compile(r'@.+?\\s')\n",
    "    no_at = re.sub(at, '', text)\n",
    "    return no_at\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def remove_stop_word(sent):\n",
    "    word_list = word_tokenize(sent)\n",
    "    word_list_filtered = [word for word in word_list if word not in stop_words]\n",
    "    word_list_lemmatized = [wnl.lemmatize(item) for item in word_list_filtered]\n",
    "    return ' '.join(word_list_lemmatized)\n",
    "\n",
    "\n",
    "\n",
    "# combine all sentences into one list\n",
    "sentence_list = []\n",
    "sentence_list_filtered = []\n",
    "for i in np.arange(df.shape[0]):\n",
    "    curr_corpus = df.loc[i, \"OriginalTweet\"]\n",
    "    curr_senten_list = sent_tokenize(curr_corpus)\n",
    "    # lemmatize + stop_words\n",
    "    curr_senten_list_cleaned = [remove_at(\n",
    "        remove_urls(\n",
    "            remove_html(\n",
    "                remove_num(\n",
    "                    remove_punctuation(\n",
    "                        lower(\n",
    "                            remove_stop_word(item)\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ) for item in curr_senten_list]\n",
    "    sentence_list.extend(curr_senten_list)\n",
    "    sentence_list_filtered.extend(curr_senten_list_cleaned)\n",
    "\n",
    "\n",
    "tfidf_sum = TfidfVectorizer(\n",
    "    norm = \"l2\", # The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "    stop_words = 'english', # already filtered\n",
    "    preprocessor = None, \n",
    "    lowercase = True, # already lowered\n",
    "    max_df = 50, \n",
    "    min_df = 20, \n",
    "    ngram_range = (1,1)\n",
    ")\n",
    "tfidf_word = tfidf_sum.fit_transform(df[\"Tweet_filtered\"])\n",
    "vocab_sum = tfidf_sum.get_feature_names()\n",
    "vocab_sum = [x.replace(' ', '_') for x in vocab_sum]\n",
    "\n",
    "\n",
    "# sum all the tfidf score for each word\n",
    "tfidf_word_all_doc = pd.Series(\n",
    "    np.array(tfidf_word.sum(axis = 0)).reshape(-1, 1).ravel(), \n",
    "    index = vocab_sum\n",
    ")\n",
    "\n",
    "# set a threshold to filter out the word that are not important\n",
    "threshold = tfidf_word_all_doc.median()\n",
    "print(f'TF-IDF threshold: {threshold:0.4f}')\n",
    "\n",
    "# for each sentence get the sentence centroid score by summing up the word score\n",
    "def get_centroid_score(sent_):\n",
    "    score = 0\n",
    "    word_list = sent_tokenize(sent_)\n",
    "    for word in word_list:\n",
    "        if word in vocab_sum: # get rid of some mis-tokenized words\n",
    "            if tfidf_word_all_doc[word] >= threshold: # TFIDF scores are below a threshold are removed\n",
    "                score += tfidf_word_all_doc[word]\n",
    "    return score\n",
    "\n",
    "# get the sentence score by filtered sentence\n",
    "sentence_score_dict = {}\n",
    "for i, curr_senten in enumerate(sentence_list_filtered):\n",
    "    sentence_score_dict[sentence_list[i]] = get_centroid_score(curr_senten)\n",
    "\n",
    "# get the ranked tweet score\n",
    "tweet_score_dict = {}\n",
    "for i, curr_senten in enumerate(sentence_list_filtered):\n",
    "    sentence_score_dict[sentence_list[i]] = get_centroid_score(curr_senten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ['19', 'amp', 'co', 'consumer', 'coronavirus', 'covid', 'covid19', 'food', 'grocery', 'https', 'people', 'prices', 'store', 'supermarket']\n",
      "Summary: \n",
      "#coronavirus #COVID19 #toiletpaper #apocalypse2020Had to fetch some forgotten items from the grocery store &amp; SHOCKED by the # people out &amp; about.\n",
      "#coronavirus #Covfefe19 #coronapocalypseThe beginning of our week of confinement https://t.co/m8emJT5xPe We have, as of yet, to be in a grocery store or other public place.\n",
      "We will help you get your restaurant or grocery store in compliance with Covid 19.\n",
      "#Report #COVID19 #ContingenyPlanning :  https://t.co/j66kLlka5h https://t.co/8cx61kXAC9@USAttyBash @TheJusticeDept Hmmm... A worker in a San Antonio grocery store has tested positive for Covid 19.\n",
      "#coronapocalypse #coronavirus #NationalEmergency #COVID19 #QuarantineAndChillRest assured, we have plenty of food that will continue to reach grocery stores on a regular basis as we continue to respond to #COVID?19.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#coronavirus #COVID19 #toiletpaper #apocalypse2020Had to fetch some forgotten items from the grocery store &amp; SHOCKED by the # people out &amp; about.\\n#coronavirus #Covfefe19 #coronapocalypseThe beginning of our week of confinement https://t.co/m8emJT5xPe We have, as of yet, to be in a grocery store or other public place.\\nWe will help you get your restaurant or grocery store in compliance with Covid 19.\\n#Report #COVID19 #ContingenyPlanning :  https://t.co/j66kLlka5h https://t.co/8cx61kXAC9@USAttyBash @TheJusticeDept Hmmm... A worker in a San Antonio grocery store has tested positive for Covid 19.\\n#coronapocalypse #coronavirus #NationalEmergency #COVID19 #QuarantineAndChillRest assured, we have plenty of food that will continue to reach grocery stores on a regular basis as we continue to respond to #COVID?19.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize as nlkt_word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# get cosine similarity\n",
    "def similarity(a, b):\n",
    "    if np.count_nonzero(a) != 0 and np.count_nonzero(b) != 0:\n",
    "        return (a.T @ b) / (norm(a)*norm(b))\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "def processing_sentence(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences_cleaned = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        words = [w for w in words if w not in string.punctuation]\n",
    "        words = [w for w in words if not w.lower() in stop_words]\n",
    "        words = [w.lower() for w in words]\n",
    "        sentences_cleaned.append(\" \".join(words))\n",
    "    return sentences_cleaned\n",
    "\n",
    "def get_tf_idf(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    sent_word_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    transformer = TfidfTransformer(\n",
    "        norm = None, \n",
    "        sublinear_tf = False, \n",
    "        smooth_idf = False\n",
    "    )\n",
    "    tfidf = transformer.fit_transform(sent_word_matrix)\n",
    "    tfidf = tfidf.toarray()\n",
    "\n",
    "    centroid_vector = tfidf.sum(0)\n",
    "    centroid_vector = np.divide(centroid_vector, centroid_vector.max())\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    relevant_vector_indices = np.where(centroid_vector > 0.3)[0]\n",
    "\n",
    "    word_list = list(np.array(feature_names)[relevant_vector_indices])\n",
    "    return word_list\n",
    "\n",
    "#Populate word vector with all embeddings.\n",
    "#This word vector is a look up table that is used\n",
    "#for getting the centroid and sentences embedding representation.\n",
    "def word_vectors_cache(sentences, embedding_model):\n",
    "    word_vectors = dict()\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        for w in words:\n",
    "            word_vectors.update({w: embedding_model.wv[w]})\n",
    "    return word_vectors\n",
    "\n",
    "# Sentence embedding representation with sum of word vectors\n",
    "def build_embedding_representation(words, word_vectors, embedding_model):\n",
    "    embedding_representation = np.zeros(embedding_model.vector_size, dtype = \"float32\")\n",
    "    word_vectors_keys = set(word_vectors.keys())\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in word_vectors_keys:\n",
    "            embedding_representation = embedding_representation + word_vectors[w]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "       embedding_representation = np.divide(embedding_representation, count)\n",
    "    return embedding_representation\n",
    "\n",
    "def summarize(text, emdedding_model):\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    clean_sentences = processing_sentence(text)\n",
    "    # for i, s in enumerate(raw_sentences):\n",
    "    #     print(i, s)\n",
    "    # for i, s in enumerate(clean_sentences):\n",
    "    #     print(i, s)\n",
    "    centroid_words = get_tf_idf(clean_sentences)\n",
    "    print(len(centroid_words), centroid_words)\n",
    "    word_vectors = word_vectors_cache(clean_sentences, emdedding_model)\n",
    "    #Centroid embedding representation\n",
    "    centroid_vector = build_embedding_representation(centroid_words, word_vectors, emdedding_model)\n",
    "    sentences_scores = []\n",
    "    for i in range(len(clean_sentences)):\n",
    "        scores = []\n",
    "        words = clean_sentences[i].split()\n",
    "\n",
    "        #Sentence embedding representation\n",
    "        sentence_vector = build_embedding_representation(words, word_vectors, emdedding_model)\n",
    "\n",
    "        #Cosine similarity between sentence embedding and centroid embedding\n",
    "        score = similarity(sentence_vector, centroid_vector)\n",
    "        sentences_scores.append((i, raw_sentences[i], score, sentence_vector))\n",
    "    sentence_scores_sort = sorted(sentences_scores, key=lambda el: el[2], reverse=True)\n",
    "    # for s in sentence_scores_sort:\n",
    "    #     print(s[0], s[1], s[2])\n",
    "    count = 0\n",
    "    sentences_summary = []\n",
    "    #Handle redundancy\n",
    "    for s in sentence_scores_sort:\n",
    "        if count > 100:\n",
    "            break\n",
    "        include_flag = True\n",
    "        for ps in sentences_summary:\n",
    "            sim = similarity(s[3], ps[3])\n",
    "            if sim > 0.95:\n",
    "                include_flag = False\n",
    "        if include_flag:\n",
    "            sentences_summary.append(s)\n",
    "            count += len(s[1].split())\n",
    "\n",
    "        sentences_summary = sorted(sentences_summary, key=lambda el: el[0], reverse = False)\n",
    "\n",
    "    summary = \"\\n\".join([s[1] for s in sentences_summary])\n",
    "    print(f'Summary: \\n{summary}')\n",
    "    return summary\n",
    "\n",
    "text = \"\"\n",
    "for i in np.arange(df.shape[0]):\n",
    "    text += df.loc[i, \"OriginalTweet\"]\n",
    "\n",
    "clean_sentences = processing_sentence(text)\n",
    "words = []\n",
    "for sent in clean_sentences:\n",
    "    words.append(nlkt_word_tokenize(sent))\n",
    "model = Word2Vec(words, min_count = 1, sg = 1) # training by: skip-gram w/ no word ignored\n",
    "summarize(text, model)\n",
    "# 7min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# save the sentence_score_dict\n",
    "import pickle\n",
    "with open('centroid_based_sentence_score_ver1.pkl', 'wb') as file:\n",
    "    pickle.dump(sentence_score_dict, file)\n",
    "file.close()\n",
    "'''\n",
    "ver1:\n",
    "    norm = \"l2\", # The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "    stop_words = 'english', # already filtered\n",
    "    preprocessor = None, \n",
    "    lowercase = True, # already lowered\n",
    "    max_df = 0.001, \n",
    "    min_df = 10, \n",
    "    ngram_range = (1,10)\n",
    "\n",
    "''' \n",
    "print(f'Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he was so excited ?? 36.30346874416296\n",
      "Excited! 36.30346874416296\n",
      "RETWEET! 34.402026556188666\n",
      "is desperate. 33.86312101050457\n",
      "Paracetamol only. 33.65921451542796\n",
      "RN: ? 33.495131851648246\n",
      "Carers. 33.42546531233133\n",
      "WONDERFUL! 33.353029765498775\n",
      "Wonderful! 33.353029765498775\n",
      "Lovely. 32.3038066601519\n",
      "Brilliant! 32.23556114394227\n",
      "Chocolate \n",
      "2. 31.561648314067686\n",
      "Sleep more.\" 31.155661191813007\n",
      "SUSPENDED. 30.969638716026427\n",
      "Bottled. 30.959285328199456\n",
      "Duty. 30.738654453089755\n",
      "Ya. 30.616861919976646\n",
      "Ca 30.289249793340876\n",
      "Locked in too? 30.269070270454932\n",
      "Bless them all. 29.90517164178371\n",
      "Bless you! 29.90517164178371\n",
      "Bless you :) 29.90517164178371\n",
      "Immunity ?. 29.605360953515095\n",
      "Bitch??? 29.110643928328095\n",
      "LIFT! 28.871270288250685\n",
      "Luck? 28.550359398544362\n",
      "Overall. 28.48822775364674\n",
      "but angry ? 26.714676490627426\n",
      "Dude... 26.55934987072801\n",
      "Publix. 26.242056201467715\n",
      "Ffs 25.54760887409805\n",
      "FFS. 25.54760887409805\n",
      "Plea 25.50292462042094\n",
      "ET. 25.493171967816245\n",
      "y tho ? 25.31744363451076\n",
      "Film at 11! 24.536796742792294\n",
      "Pass it on. 23.868290211701893\n",
      "a Uber ? 23.803319928108817\n",
      "Hang in there. 23.598597158200782\n",
      "Unique. 23.548520150595095\n",
      "Genuinely. ? 23.464047760239254\n",
      "Shocking!! 23.18423947815464\n",
      "Shocking. 23.18423947815464\n",
      "Shocking the no. 23.18423947815464\n",
      "FL. 22.878246810975682\n",
      "Clue me in. 22.808249757419645\n",
      "Awful! 22.786321677855124\n",
      "Jesus. 22.528533560756472\n",
      "Jesus ? 22.528533560756472\n",
      "Bed. 22.489770887962738\n"
     ]
    }
   ],
   "source": [
    "# sort by importance value in descending order\n",
    "sentence_score_dict_sorted = {k: v for k, v in sorted(\n",
    "    sentence_score_dict.items(), key=lambda item: item[1], \n",
    "    reverse = True\n",
    ")[:50]}\n",
    "\n",
    "for sentence, score in sentence_score_dict_sorted.items():\n",
    "    print(sentence, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Document-level sentiment classification\n",
    "* Use stacking\n",
    "    First layer: XGBoost / RF / AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11057\n",
       "Negative               9740\n",
       "Extremely Positive     6928\n",
       "Extremely Negative     5766\n",
       "Neutral                5738\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1    0.281858\n",
      "-1    0.248286\n",
      " 2    0.176604\n",
      "-2    0.146983\n",
      " 0    0.146269\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sentiment_encode= {\n",
    "    'Extremely Negative': -2, \n",
    "    'Negative': -1, \n",
    "    'Neutral': 0,\n",
    "    'Positive': 1,\n",
    "    'Extremely Positive': 2\n",
    "}\n",
    "y = df.Sentiment.replace(sentiment_encode)\n",
    "print(y.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.Tweet_filtered, y, \n",
    "    test_size = 0.2, \n",
    "    stratify = y, \n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build pipeline\n",
    "* tfidf + truncated SVD(pca) + lgb classifier\n",
    "* tfidf + truncated SVD(pca) + stackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=20, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/kangshuoli/miniforge3/envs/eods-s22/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=100, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "[CV] END lgbc__importance_type=gain, lgbc__learning_rate=0.005, lgbc__max_depth=50, lgbc__n_estimators=200, lgbc__n_jobs=-1, lgbc__num_iterations=3000, lgbc__random_state=42, svd__n_components=100, svd__random_state=42, tfidf__max_df=0.5, tfidf__min_df=30; total time= 3.3min\n",
      "Best param: \n",
      "{'lgbc__importance_type': 'gain', 'lgbc__learning_rate': 0.005, 'lgbc__max_depth': 20, 'lgbc__n_estimators': 100, 'lgbc__n_jobs': -1, 'lgbc__num_iterations': 3000, 'lgbc__random_state': 42, 'svd__n_components': 100, 'svd__random_state': 42, 'tfidf__max_df': 0.5, 'tfidf__min_df': 30}\n",
      "Best socre: \n",
      "0.4039\n",
      "Accuracy Score:0.4052\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe_lgbc = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svd', TruncatedSVD()), \n",
    "    ('lgbc', LGBMClassifier())\n",
    "])\n",
    "\n",
    "n_components_list = [int(item) for item in np.logspace(0.2, 2.5, num = 20)]\n",
    "\n",
    "\n",
    "param_grid_lgbc = {\n",
    "    'tfidf__min_df':[30],\n",
    "    'tfidf__max_df':[0.4],\n",
    "    'svd__n_components':n_components_list,\n",
    "    'svd__random_state':[42],\n",
    "    'lgbc__n_estimators':[100],\n",
    "    'lgbc__num_iterations':[800],\n",
    "    'lgbc__max_depth':[50, 100], \n",
    "    'lgbc__learning_rate':[0.005], \n",
    "    'lgbc__n_jobs':[-1], \n",
    "    'lgbc__random_state':[42], \n",
    "    'lgbc__importance_type':['gain']\n",
    "}\n",
    "\n",
    "gs_pipe_lgbc = GridSearchCV(\n",
    "    pipe_lgbc, \n",
    "    param_grid_lgbc, \n",
    "    scoring = 'accuracy',\n",
    "    cv = 5, \n",
    "    n_jobs = -1, \n",
    "    verbose = 2\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "\n",
    "clf = gs_pipe_lgbc.best_estimator_\n",
    "cv_result = gs_pipe_lgbc.cv_results_\n",
    "test_score = clf.score(x_test, y_test)\n",
    "best_index = gs_pipe_lgbc.best_index_\n",
    "best_train_score = gs_pipe_lgbc.best_score_\n",
    "best_validation_score = cv_result['mean_test_score'][best_index]\n",
    "best_params = gs_pipe_lgbc.best_params_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'Best param: \\n{gs_pipe_lgbc.best_params_}')\n",
    "print(f'Best score: \\n{gs_pipe_lgbc.best_score_:0.4f}')\n",
    "\n",
    "y_predict = gs_pipe_lgbc.best_estimator_.predict(x_test)\n",
    "print(f'Test Accuracy:{accuracy_score(y_test, y_predict):.4f}')\n",
    "my_cmap = cm.get_cmap('RdBu')\n",
    "conf_mx = confusion_matrix(y_test, y_predict)\n",
    "plt.matshow(conf_mx, cmap = my_cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Current n_components: 316\n"
     ]
    }
   ],
   "source": [
    "# n_components_list = [int(item) for item in np.logspace(0.2, 2.5, num = 20)] # ver1\n",
    "n_components_list = [int(item) for item in np.logspace(2.5, 3.5, num = 15)] # ver2\n",
    "\n",
    "def get_validation_test_score_list(n_components_list):\n",
    "    validation_error_list = []\n",
    "    test_error_list = []\n",
    "    for value in n_components_list:\n",
    "        print(f'--------------------------------------')\n",
    "        print(f'Current n_components: {value}')\n",
    "        pipe_lgbc = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('svd', TruncatedSVD()), \n",
    "            ('lgbc', LGBMClassifier())\n",
    "        ])\n",
    "        \n",
    "        param_grid_lgbc = {\n",
    "            'tfidf__min_df':[30],\n",
    "            'tfidf__max_df':[0.4],\n",
    "            'svd__n_components':[value],\n",
    "            'svd__random_state':[42],\n",
    "            'lgbc__n_estimators':[200],\n",
    "            'lgbc__num_iterations':[600],\n",
    "            'lgbc__max_depth':[50, 100], \n",
    "            'lgbc__learning_rate':[0.01], \n",
    "            'lgbc__n_jobs':[-1], \n",
    "            'lgbc__random_state':[42], \n",
    "            'lgbc__importance_type':['gain']\n",
    "        }\n",
    "\n",
    "        gs_pipe_lgbc = GridSearchCV(\n",
    "            pipe_lgbc, \n",
    "            param_grid_lgbc, \n",
    "            scoring = 'accuracy',\n",
    "            cv = 5, \n",
    "            n_jobs = -1, \n",
    "            verbose = 0, \n",
    "            return_train_score = True\n",
    "        ).fit(x_train, y_train)\n",
    "\n",
    "\n",
    "        clf = gs_pipe_lgbc.best_estimator_\n",
    "        cv_result = gs_pipe_lgbc.cv_results_\n",
    "        temp_test_score = clf.score(x_test, y_test)\n",
    "        # temp_best_index = gs_pipe_lgbc.best_index_\n",
    "        temp_best_validation_score = gs_pipe_lgbc.best_score_\n",
    "        # temp_best_validation_score = cv_result['mean_test_score'][temp_best_index]\n",
    "        temp_best_params = gs_pipe_lgbc.best_params_\n",
    "\n",
    "        result_df = pd.DataFrame(\n",
    "            data = [[temp_best_validation_score],\n",
    "                    [temp_test_score]], \n",
    "            columns = [\"scores\"], \n",
    "            index = [\"Validation\", \"Test\"]\n",
    "        )\n",
    "        display(result_df)\n",
    "\n",
    "        validation_error_list.append(temp_best_validation_score)\n",
    "        test_error_list.append(temp_test_score)\n",
    "    return n_components_list, validation_error_list, test_error_list\n",
    "\n",
    "def plot_learning_curve(n_var_list, validation_score_list, test_score_list):\n",
    "    plt.style.use('ggplot')\n",
    "    plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "    plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (8, 8), dpi = 320)\n",
    "    ax.plot(n_var_list, validation_score_list, 'o-', label = \"Validation score\", linewidth = 2)\n",
    "    ax.plot(n_var_list, test_score_list, 'o-', label = \"Test score\", linewidth = 2)\n",
    "    ax.legend()\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_title('Learning Curve: LGBMClassifier')\n",
    "    ax.set_xlabel('n_components in TruncatedSVD')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "n_components_list, validation_error_list, test_error_list = get_validation_test_score_list(n_components_list)\n",
    "plot_learning_curve(n_components_list, validation_error_list, test_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save learning curve\n",
    "# Ver1 -> param_grid_lgbc = {\n",
    "#             'tfidf__min_df':[30],\n",
    "#             'tfidf__max_df':[0.4],\n",
    "#             'svd__n_components':[value],\n",
    "#             'svd__random_state':[42],\n",
    "#             'lgbc__n_estimators':[200],\n",
    "#             'lgbc__num_iterations':[600],\n",
    "#             'lgbc__max_depth':[50, 100], \n",
    "#             'lgbc__learning_rate':[0.01], \n",
    "#             'lgbc__n_jobs':[-1], \n",
    "#             'lgbc__random_state':[42], \n",
    "#             'lgbc__importance_type':['gain']\n",
    "#         }\n",
    "# with open('learning_curve_ver1.pkl', 'wb') as file:\n",
    "#     pickle.dump([n_components_list, validation_error_list, test_error_list], file)\n",
    "# file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stacking = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svd', TruncatedSVD()), \n",
    "    ('stacking', StackingClassifier(\n",
    "        estimators = [\n",
    "            ('xgb', xgb.XGBClassifier(\n",
    "                n_estimator = 1000, \n",
    "                max_depth = 30, \n",
    "                learning_rate = 0.01, \n",
    "                verbosity = 0, \n",
    "                booster = 'gbtree', \n",
    "                n_jobs = -1, \n",
    "                random_state = 42, \n",
    "                reg_alpha = 0.5, \n",
    "                importance_type = \"gain\"\n",
    "            )), \n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators = 600, \n",
    "                max_depth = 20, \n",
    "                random_state = 42, \n",
    "                n_jobs = -1\n",
    "            )), \n",
    "            ('ada', AdaBoostClassifier(\n",
    "                n_estimators = 600, \n",
    "                random_state = 42, \n",
    "                n_jobs = -1\n",
    "            )), \n",
    "            ('lr', LogisticRegression(\n",
    "                penalty = 'l1', \n",
    "                dual = True, \n",
    "                C = 1, \n",
    "                random_state = 42, \n",
    "                n_jobs = -1\n",
    "            ))\n",
    "        ]\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid_lgbc = {\n",
    "    'tfidf__min_df':[10, 20, 30],\n",
    "    'tfidf__max_df':[0.2, 0.3, 0.4],\n",
    "    'svd__n_components':[100, 200],\n",
    "\n",
    "}\n",
    "\n",
    "gs_pipe_lgbc = GridSearchCV(\n",
    "    pipe_lgbc, \n",
    "    param_grid_lgbc, \n",
    "    scoring = 'accuracy',\n",
    "    cv = 5, \n",
    "    n_jobs=-1\n",
    ").fit(x_train, y_train)\n",
    "\n",
    "print(f'Best param: \\n{gs_pipe_lgbc.best_params_}')\n",
    "print(f'Best socre: \\n{gs_pipe_lgbc.best_score_:0.4f}')\n",
    "\n",
    "y_predict = gs_pipe_lgbc.best_estimator_.predict(x_test)\n",
    "print(f'Accuracy Score:{accuracy_score(y_test, y_predict):.4f}')\n",
    "my_cmap = cm.get_cmap('RdBu')\n",
    "conf_mx = confusion_matrix(y_test, y_predict)\n",
    "plt.matshow(conf_mx, cmap = my_cmap)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27646e5cfdb25e10f4fd8ef0a3a49281af2a8af5fb226dbbb4749ab5404e27a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('eods-s22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
