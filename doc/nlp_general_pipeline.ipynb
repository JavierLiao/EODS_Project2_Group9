{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GR5293 - Proj2 - Group9\n",
    "## NLP with tweets related to COVID\n",
    "#### NLP pipeline with sentiment prediction\n",
    "* Tokenization\n",
    "    > Split text into tokens(sentences or words), for this question, we split the document into sentence for automatic summarization, and words for sentiment analysis and topic modeling\n",
    "* Screen out stop words and other meaningless corpus\n",
    "* Lemmatization\n",
    "    > Here we only use lemmatization rather than stemming is because lemmatization keeps the interpretability of words with their context. While stemming might lead to incorrect meaning. It is important to make morphological analysis of the words. \n",
    "* EDA: wordCloud with different sentiment\n",
    "    > Identify what poeple with different emotions were considering about\n",
    "* EDA: Word2vec with Clustering\n",
    "    > Word2Vec: Effective for detecting the synonymous words or suggesting additional words for a partial sentence\n",
    "\n",
    "    Clustering methods: K-means + DBScan\n",
    "\n",
    "    Use all the words in a specific part-of-speech from all the documents (e.g. all nouns / all adj.s)\n",
    "* (word2vec w/ recommendation)?\n",
    "* Topic Modeling: Feature extraction by TFIDF + Latent Dirichlet Allocation\n",
    "    Build a pipeline with KFoldCV to find the best topic number\n",
    "* Automatic summrization\n",
    "    > Identify what were most people thinking about or tweeting for\n",
    "* Sentiment Analysis: Classification for sentiment(5 classes: Neutral / Positive / Extremely Positive / Negative / Extremely Negative)\n",
    "  \n",
    "    Potential Model: lightGBM / stacking / BERT?\n",
    "#### Preprocessing\n",
    "* One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/kangshuoli/Documents/VScode_workspace/GR5293/EODS-Project2-Group9/EODS_Project2_Group9/doc'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from sklearn import pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%xmode plain\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_filtered</th>\n",
       "      <th>Word_list</th>\n",
       "      <th>Senten_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice talk neighbour family exchange phone nu...</td>\n",
       "      <td>['advice', 'talk', 'neighbour', 'family', 'exc...</td>\n",
       "      <td>['advice talk to your neighbours family to exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>coronavirus australia woolworth give elderly d...</td>\n",
       "      <td>['coronavirus', 'australia', 'woolworth', 'giv...</td>\n",
       "      <td>['coronavirus australia: woolworths to give el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>food stock one empty please dont panic enough ...</td>\n",
       "      <td>['food', 'stock', 'one', 'empty', 'please', 'd...</td>\n",
       "      <td>['my food stock is not the only one which is e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>ready go supermarket covid outbreak im paranoi...</td>\n",
       "      <td>['ready', 'go', 'supermarket', 'covid', 'outbr...</td>\n",
       "      <td>['me, ready to go at supermarket during the #c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>news regionâ  first confirmed covid case came...</td>\n",
       "      <td>['news', 'regionâ', '\\x92', 'first', 'confirme...</td>\n",
       "      <td>['as news of the regionâ\\x92s first confirmed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44248</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>meanwhile supermarket israel people dance sing...</td>\n",
       "      <td>['meanwhile', 'supermarket', 'israel', 'people...</td>\n",
       "      <td>['meanwhile in a supermarket in israel -- peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44249</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>panic buy lot nonperishable item echo need foo...</td>\n",
       "      <td>['panic', 'buy', 'lot', 'nonperishable', 'item...</td>\n",
       "      <td>['did you panic buy a lot of non-perishable it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44250</th>\n",
       "      <td>Asst Prof of Economics @cconces was on @NBCPhi...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>asst prof economics talking recent research co...</td>\n",
       "      <td>['asst', 'prof', 'economics', 'talking', 'rece...</td>\n",
       "      <td>[\"asst prof of economics @cconces was on @nbcp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44251</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>gov need somethings instead biar je rakyat ass...</td>\n",
       "      <td>['gov', 'need', 'somethings', 'instead', 'biar...</td>\n",
       "      <td>[\"gov need to do somethings instead of biar je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44252</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>member committed safety employee endusers moni...</td>\n",
       "      <td>['member', 'committed', 'safety', 'employee', ...</td>\n",
       "      <td>['i and @forestandpaper members are committed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44253 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet           Sentiment  \\\n",
       "0      advice Talk to your neighbours family to excha...            Positive   \n",
       "1      Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "2      My food stock is not the only one which is emp...            Positive   \n",
       "3      Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "4      As news of the regionÂs first confirmed COVID...            Positive   \n",
       "...                                                  ...                 ...   \n",
       "44248  Meanwhile In A Supermarket in Israel -- People...            Positive   \n",
       "44249  Did you panic buy a lot of non-perishable item...            Negative   \n",
       "44250  Asst Prof of Economics @cconces was on @NBCPhi...             Neutral   \n",
       "44251  Gov need to do somethings instead of biar je r...  Extremely Negative   \n",
       "44252  I and @ForestandPaper members are committed to...  Extremely Positive   \n",
       "\n",
       "                                          Tweet_filtered  \\\n",
       "0      advice talk neighbour family exchange phone nu...   \n",
       "1      coronavirus australia woolworth give elderly d...   \n",
       "2      food stock one empty please dont panic enough ...   \n",
       "3      ready go supermarket covid outbreak im paranoi...   \n",
       "4      news regionâ  first confirmed covid case came...   \n",
       "...                                                  ...   \n",
       "44248  meanwhile supermarket israel people dance sing...   \n",
       "44249  panic buy lot nonperishable item echo need foo...   \n",
       "44250  asst prof economics talking recent research co...   \n",
       "44251  gov need somethings instead biar je rakyat ass...   \n",
       "44252  member committed safety employee endusers moni...   \n",
       "\n",
       "                                               Word_list  \\\n",
       "0      ['advice', 'talk', 'neighbour', 'family', 'exc...   \n",
       "1      ['coronavirus', 'australia', 'woolworth', 'giv...   \n",
       "2      ['food', 'stock', 'one', 'empty', 'please', 'd...   \n",
       "3      ['ready', 'go', 'supermarket', 'covid', 'outbr...   \n",
       "4      ['news', 'regionâ', '\\x92', 'first', 'confirme...   \n",
       "...                                                  ...   \n",
       "44248  ['meanwhile', 'supermarket', 'israel', 'people...   \n",
       "44249  ['panic', 'buy', 'lot', 'nonperishable', 'item...   \n",
       "44250  ['asst', 'prof', 'economics', 'talking', 'rece...   \n",
       "44251  ['gov', 'need', 'somethings', 'instead', 'biar...   \n",
       "44252  ['member', 'committed', 'safety', 'employee', ...   \n",
       "\n",
       "                                             Senten_list  \n",
       "0      ['advice talk to your neighbours family to exc...  \n",
       "1      ['coronavirus australia: woolworths to give el...  \n",
       "2      ['my food stock is not the only one which is e...  \n",
       "3      ['me, ready to go at supermarket during the #c...  \n",
       "4      ['as news of the regionâ\\x92s first confirmed ...  \n",
       "...                                                  ...  \n",
       "44248  ['meanwhile in a supermarket in israel -- peop...  \n",
       "44249  ['did you panic buy a lot of non-perishable it...  \n",
       "44250  [\"asst prof of economics @cconces was on @nbcp...  \n",
       "44251  [\"gov need to do somethings instead of biar je...  \n",
       "44252  ['i and @forestandpaper members are committed ...  \n",
       "\n",
       "[44253 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Corona_NLP_filtered.csv\")\n",
    "df.drop(\n",
    "    \"Unnamed: 0\", \n",
    "    axis = 1,\n",
    "    inplace = True\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44253 entries, 0 to 44252\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   OriginalTweet   44253 non-null  object\n",
      " 1   Sentiment       44251 non-null  object\n",
      " 2   Tweet_filtered  44251 non-null  object\n",
      " 3   Word_list       44250 non-null  object\n",
      " 4   Senten_list     44249 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "* Most done in data_cleaning.ipynb\n",
    "* Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(\n",
    "    axis = 0, \n",
    "    how = \"any\", \n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 44249 entries, 0 to 44252\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   OriginalTweet   44249 non-null  object\n",
      " 1   Sentiment       44249 non-null  object\n",
      " 2   Tweet_filtered  44249 non-null  object\n",
      " 3   Word_list       44249 non-null  object\n",
      " 4   Senten_list     44249 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-level Analysis\n",
    "1. wordCloud\n",
    "2. word2Vec w/ clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-level / Sentence-level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentence-level automatic summarization\n",
    "* Extractive summarization: pick the original sentence which can represent the main focus of the whole document\n",
    "* Abstractive summarization: generate new sentences for summary\n",
    "    > Purely extractive summaries often times give better results compared to automatic abstractive summaries. This is because of the fact that abstractive summarization methods cope with problems such as semantic representation, inference and natural language generation which are relatively harder than data-driven approaches such as sentence extraction.\n",
    "\n",
    "We use frequency-driven approch here. \n",
    "\n",
    "##### Extractive method w/ TFIDF\n",
    "##### Model: Centroid-based summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Centroid-based summarization\n",
    "all_sentence_list = []\n",
    "for i in np.arange(df.shape[0]):\n",
    "    all_sentence_list.extend(df.loc[i, \"Senten_list\"])\n",
    "\n",
    "# sentence cleaning\n",
    "\n",
    "\n",
    "\n",
    "# tfidf_sum = TfidfVectorizer(\n",
    "#     norm = \"l2\", # The cosine similarity between two vectors is their dot product when l2 norm has been applied.\n",
    "#     stop_words = None, # already filtered\n",
    "#     preprocessor = None, \n",
    "#     lowercase = False, # already lowered\n",
    "#     max_df = 0.9, \n",
    "#     min_df = 10, \n",
    "#     ngram_range = (1,5)\n",
    "# )\n",
    "# tfidf_df = tfidf_sum.fit_transform(df[\"Tweet_filtered\"])\n",
    "# vocab_sum = tfidf_sum.get_feature_names()\n",
    "# vocab_sum = [x.replace(' ', '_') for x in vocab_sum]\n",
    "\n",
    "# # Set a threshold to filter the word that are important\n",
    "# threshold = 0.5\n",
    "\n",
    "# # Construct the centroid of clusters, use tfidf score as the centroid score\n",
    "# mask_df = tfidf_df < threshold # sparse matrix\n",
    "# print(type(tfidf_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44249, 13765)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape # total 13765 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculates cosine similarity\n",
    "# def similarity(v1, v2):\n",
    "#     score = 0.0\n",
    "#     if np.count_nonzero(v1) != 0 and np.count_nonzero(v2) != 0:\n",
    "#         score = ((1 - cosine(v1, v2)) + 1) / 2\n",
    "#     return score\n",
    "\n",
    "# def get_tf_idf(sentences):\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     sent_word_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "#     transformer = TfidfTransformer(norm=None, sublinear_tf=False, smooth_idf=False)\n",
    "#     tfidf = transformer.fit_transform(sent_word_matrix)\n",
    "#     tfidf = tfidf.toarray()\n",
    "\n",
    "#     centroid_vector = tfidf.sum(0)\n",
    "#     centroid_vector = np.divide(centroid_vector, centroid_vector.max())\n",
    "\n",
    "#     feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "#     relevant_vector_indices = np.where(centroid_vector > 0.3)[0]\n",
    "\n",
    "#     word_list = list(np.array(feature_names)[relevant_vector_indices])\n",
    "#     return word_list\n",
    "\n",
    "# #Populate word vector with all embeddings.\n",
    "# #This word vector is a look up table that is used\n",
    "# #for getting the centroid and sentences embedding representation.\n",
    "# def word_vectors_cache(sentences, embedding_model):\n",
    "#     word_vectors = dict()\n",
    "#     for sent in sentences:\n",
    "#         words = nlkt_word_tokenize(sent)\n",
    "#         for w in words:\n",
    "#             word_vectors.update({w: embedding_model.wv[w]})\n",
    "#     return word_vectors\n",
    "\n",
    "# # Sentence embedding representation with sum of word vectors\n",
    "# def build_embedding_representation(words, word_vectors, embedding_model):\n",
    "#     embedding_representation = np.zeros(embedding_model.vector_size, dtype=\"float32\")\n",
    "#     word_vectors_keys = set(word_vectors.keys())\n",
    "#     count = 0\n",
    "#     for w in words:\n",
    "#         if w in word_vectors_keys:\n",
    "#             embedding_representation = embedding_representation + word_vectors[w]\n",
    "#             count += 1\n",
    "#     if count != 0:\n",
    "#        embedding_representation = np.divide(embedding_representation, count)\n",
    "#     return embedding_representation\n",
    "\n",
    "# def summarize(text, emdedding_model):\n",
    "#     raw_sentences = sent_tokenize(text)\n",
    "#     clean_sentences = cleanup_sentences(text)\n",
    "#     for i, s in enumerate(raw_sentences):\n",
    "#         print(i, s)\n",
    "#     for i, s in enumerate(clean_sentences):\n",
    "#         print(i, s)\n",
    "#     centroid_words = get_tf_idf(clean_sentences)\n",
    "#     print(len(centroid_words), centroid_words)\n",
    "#     word_vectors = word_vectors_cache(clean_sentences, emdedding_model)\n",
    "#     #Centroid embedding representation\n",
    "#     centroid_vector = build_embedding_representation(centroid_words, word_vectors, emdedding_model)\n",
    "#     sentences_scores = []\n",
    "#     for i in range(len(clean_sentences)):\n",
    "#         scores = []\n",
    "#         words = clean_sentences[i].split()\n",
    "\n",
    "#         #Sentence embedding representation\n",
    "#         sentence_vector = build_embedding_representation(words, word_vectors, emdedding_model)\n",
    "\n",
    "#         #Cosine similarity between sentence embedding and centroid embedding\n",
    "#         score = similarity(sentence_vector, centroid_vector)\n",
    "#         sentences_scores.append((i, raw_sentences[i], score, sentence_vector))\n",
    "#     sentence_scores_sort = sorted(sentences_scores, key=lambda el: el[2], reverse=True)\n",
    "#     for s in sentence_scores_sort:\n",
    "#         print(s[0], s[1], s[2])\n",
    "#     count = 0\n",
    "#     sentences_summary = []\n",
    "#     #Handle redundancy\n",
    "#     for s in sentence_scores_sort:\n",
    "#         if count > 100:\n",
    "#             break\n",
    "#         include_flag = True\n",
    "#         for ps in sentences_summary:\n",
    "#             sim = similarity(s[3], ps[3])\n",
    "#             if sim > 0.95:\n",
    "#                 include_flag = False\n",
    "#         if include_flag:\n",
    "#             sentences_summary.append(s)\n",
    "#             count += len(s[1].split())\n",
    "\n",
    "#         sentences_summary = sorted(sentences_summary, key=lambda el: el[0], reverse=False)\n",
    "\n",
    "#     summary = \"\\n\".join([s[1] for s in sentences_summary])\n",
    "#     print(summary)\n",
    "#     return summary\n",
    "\n",
    "\n",
    "# clean_sentences = cleanup_sentences(text)\n",
    "# words = []\n",
    "# for sent in clean_sentences:\n",
    "#     words.append(nlkt_word_tokenize(sent))\n",
    "# model = Word2Vec(words, min_count=1, sg = 1)\n",
    "# summarize(text, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Document-level sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27646e5cfdb25e10f4fd8ef0a3a49281af2a8af5fb226dbbb4749ab5404e27a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('eods-s22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
